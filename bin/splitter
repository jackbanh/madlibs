#!/usr/bin/ruby

require 'parallel'
require_relative '../lib/phrase_bucketer.rb'

MAX_TEXTS = 1000

corpuses = []

# Get all subdirectories under texts
dirs = Dir['./texts/*/']

dirs.each do |d|
  name = d.split("/").last
  bucketer = PhraseBucketer.new(name)

  # Corpus is a list of all text files inside the subdirectory
  corpus = Dir.glob("#{d}*.txt")

  next if corpus.empty?

  # Pick a random sample of files
  files = corpus.sample(MAX_TEXTS)

  # In parallel, read it each file and generate a list of a separate phrases and nouns
  options = {
    progress => "Reading #{name} files..."
  }
  texts = Parallel.map(files, options) do |file|
    bucketer.process_text(File.read file)
  end

  # Merge all the separate texts into the bucketer
  texts.reduce(bucketer) do |memo, text|
    phrase_buckets, removed_nouns = text
    memo.merge_phrase_buckets_and_nouns phrase_buckets, removed_nouns
  end

  corpuses.push(bucketer)
end

Dir.mkdir "output" unless Dir.exist?("output")

corpuses.each do |bucket|
  # Write _1.txt and _2.txt files
  # phrase_buckets will have 2 buckets: 1 or 2 nouns to replace
  bucket.phrase_buckets.each do |bucket_number, phrases|
    bucket_file = "output/#{bucket.name}_#{bucket_number}.txt"

    File.open(bucket_file, "w") { |file| file.puts(phrases.to_a) }
    puts "Wrote #{bucket_file}."
  end

  # Write removed nouns
  noun_file = "output/#{bucket.name}_removed_nouns.txt"

  File.open(noun_file, "w") { |file| file.puts(bucket.removed_nouns.to_a) }
  puts "Wrote #{noun_file}."
end